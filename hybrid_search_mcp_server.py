#!/usr/bin/env python3

"""
Hybrid Search MCP Server - FastMCP Implementation

FastMCPã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢MCPã‚µãƒ¼ãƒãƒ¼
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ç›£è¦–æ©Ÿèƒ½ä»˜ã
"""

import json
import logging
from pathlib import Path
import sys
import time
import threading
from typing import Dict, Any

# ãƒ‘ã‚¹ã®è¨­å®šï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

# FastMCPãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from fastmcp import FastMCP

# Watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ç”¨ï¼‰
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    logging.warning("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‡ªå‹•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°æ©Ÿèƒ½ãŒç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
    logging.warning("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install watchdog")

# æ—¢å­˜ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
from hybrid_search_engine import HybridSearchEngine
from hybrid_index_manager import HybridIndexManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
search_engine: HybridSearchEngine = None
index_watcher = None


class IndexFileWatcher:
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    Watchdogã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ™ã‚¯ãƒˆãƒ«DBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–ã—ã€
    å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã«è‡ªå‹•çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã™ã€‚
    """
    
    def __init__(self, search_engine: HybridSearchEngine):
        """
        IndexFileWatcherã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        
        Args:
            search_engine (HybridSearchEngine): æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.search_engine = search_engine
        self.observers = []
        self.running = False
        self.cooldown_period = 2.0  # é€£ç¶šæ›´æ–°ã‚’é˜²ãã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“ï¼ˆç§’ï¼‰
        self.last_reload_time = {}
        
        if not WATCHDOG_AVAILABLE:
            logger.warning("âš ï¸ WatchdogãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
            return
            
        # æ—¢å­˜ã®è‡ªå‹•å†èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        self.search_engine.set_auto_reload(True)
        
        logger.info("ğŸ”„ IndexFileWatcheråˆæœŸåŒ–å®Œäº†")
    
    def _get_watch_paths(self) -> Dict[str, Path]:
        """
        ç›£è¦–å¯¾è±¡ãƒ‘ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚
        
        Returns:
            Dict[str, Path]: ç›£è¦–å¯¾è±¡ãƒ‘ã‚¹ã®è¾æ›¸
        """
        watch_paths = {}
        
        try:
            for name, retriever in self.search_engine.index_manager.retrievers.items():
                index_info = retriever.get_index_info()
                
                if name == 'bm25' and 'index_file' in index_info:
                    # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
                    index_path = Path(index_info['index_file'])
                    if index_path.exists():
                        watch_paths[f'{name}_index'] = index_path.parent
                        logger.info(f"ğŸ“Š BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç›£è¦–å¯¾è±¡: {index_path.parent}")
                
                elif name == 'vector' and 'db_path' in index_info:
                    # ãƒ™ã‚¯ãƒˆãƒ«DBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
                    db_path = Path(index_info['db_path'])
                    if db_path.exists():
                        watch_paths[f'{name}_db'] = db_path
                        logger.info(f"ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«DBç›£è¦–å¯¾è±¡: {db_path}")
        
        except Exception as e:
            logger.error(f"ç›£è¦–ãƒ‘ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return watch_paths
    
    def start_watching(self) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™ã€‚
        
        Returns:
            bool: ç›£è¦–é–‹å§‹ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        if not WATCHDOG_AVAILABLE:
            logger.warning("âš ï¸ WatchdogãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹ã§ãã¾ã›ã‚“")
            return False
        
        if self.running:
            logger.warning("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã¯æ—¢ã«é–‹å§‹ã•ã‚Œã¦ã„ã¾ã™")
            return True
        
        try:
            watch_paths = self._get_watch_paths()
            
            if not watch_paths:
                logger.warning("âš ï¸ ç›£è¦–å¯¾è±¡ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # å„ãƒ‘ã‚¹ã«å¯¾ã—ã¦Observerã‚’ä½œæˆ
            for watch_key, watch_path in watch_paths.items():
                event_handler = IndexChangeHandler(
                    search_engine=self.search_engine,
                    watcher=self,
                    watch_key=watch_key
                )
                
                observer = Observer()
                observer.schedule(event_handler, str(watch_path), recursive=True)
                observer.start()
                
                self.observers.append(observer)
                logger.info(f"ğŸ” ç›£è¦–é–‹å§‹: {watch_key} -> {watch_path}")
            
            self.running = True
            logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹ - {len(self.observers)}å€‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–ä¸­")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            self.stop_watching()
            return False
    
    def stop_watching(self) -> None:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’åœæ­¢ã—ã¾ã™ã€‚
        """
        if not self.running:
            return
        
        try:
            for observer in self.observers:
                observer.stop()
                observer.join(timeout=5.0)
            
            self.observers.clear()
            self.running = False
            logger.info("ğŸ›‘ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–åœæ­¢")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–åœæ­¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    def should_reload(self, watch_key: str) -> bool:
        """
        ãƒªãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¾ã™ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“ã‚’è€ƒæ…®ï¼‰ã€‚
        
        Args:
            watch_key (str): ç›£è¦–ã‚­ãƒ¼
            
        Returns:
            bool: ãƒªãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆTrue
        """
        current_time = time.time()
        last_reload = self.last_reload_time.get(watch_key, 0)
        
        if current_time - last_reload < self.cooldown_period:
            return False
        
        self.last_reload_time[watch_key] = current_time
        return True
    
    def trigger_reload(self, watch_key: str) -> None:
        """
        ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        
        Args:
            watch_key (str): ç›£è¦–ã‚­ãƒ¼
        """
        try:
            logger.info(f"ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¤‰æ›´æ¤œçŸ¥ ({watch_key}) - è‡ªå‹•å†èª­ã¿è¾¼ã¿å®Ÿè¡Œä¸­...")
            results = self.search_engine.force_index_reload()
            
            successful = sum(1 for success in results.values() if success)
            total = len(results)
            
            if successful > 0:
                logger.info(f"âœ… è‡ªå‹•å†èª­ã¿è¾¼ã¿å®Œäº†: {successful}/{total}å€‹æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸ è‡ªå‹•å†èª­ã¿è¾¼ã¿çµæœ: {successful}/{total}å€‹æˆåŠŸ")
                
        except Exception as e:
            logger.error(f"âŒ è‡ªå‹•å†èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


class IndexChangeHandler(FileSystemEventHandler):
    """
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    """
    
    def __init__(self, search_engine: HybridSearchEngine, watcher: IndexFileWatcher, watch_key: str):
        """
        IndexChangeHandlerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        
        Args:
            search_engine (HybridSearchEngine): æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            watcher (IndexFileWatcher): ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
            watch_key (str): ç›£è¦–ã‚­ãƒ¼
        """
        super().__init__()
        self.search_engine = search_engine
        self.watcher = watcher
        self.watch_key = watch_key
    
    def on_modified(self, event):
        """
        ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†
        
        Args:
            event: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ™ãƒ³ãƒˆ
        """
        if event.is_directory:
            return
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã®ã¿ã‚’å‡¦ç†
        file_path = Path(event.src_path)
        
        # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ãƒã‚§ãƒƒã‚¯
        if (self.watch_key.startswith('bm25') and 
            file_path.suffix in ['.pkl', '.joblib', '.index']):
            
            logger.info(f"ğŸ“Š BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥: {file_path.name}")
            
            if self.watcher.should_reload(self.watch_key):
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å†èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œ
                threading.Thread(
                    target=self.watcher.trigger_reload,
                    args=(self.watch_key,),
                    daemon=True
                ).start()
        
        # ãƒ™ã‚¯ãƒˆãƒ«DBé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ãƒã‚§ãƒƒã‚¯
        elif (self.watch_key.startswith('vector') and 
              (file_path.suffix in ['.lance', '.manifest', '.txn'] or 
               'lance' in str(file_path))):
            
            logger.info(f"ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«DBå¤‰æ›´æ¤œçŸ¥: {file_path.name}")
            
            if self.watcher.should_reload(self.watch_key):
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å†èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œ
                threading.Thread(
                    target=self.watcher.trigger_reload,
                    args=(self.watch_key,),
                    daemon=True
                ).start()
    
    def on_created(self, event):
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚ã®å‡¦ç†ï¼ˆä¿®æ­£æ™‚ã¨åŒã˜å‡¦ç†ï¼‰
        """
        self.on_modified(event)


def initialize_search_engine():
    """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¨ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã™"""
    global search_engine, index_watcher
    
    try:
        logger.info("ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ä¸­...")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        index_manager = HybridIndexManager()
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        system_status = index_manager.get_system_status()
        if not system_status['retrievers']:
            logger.error("âŒ æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.error("hybrid_index_manager.pyã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„")
            return False
        
        # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
        search_engine = HybridSearchEngine(index_manager)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        index_watcher = IndexFileWatcher(search_engine)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹
        watch_success = index_watcher.start_watching()
        
        # æº–å‚™çŠ¶æ…‹ã‚’ç¢ºèª
        ready_retrievers = []
        for name, retriever_status in system_status['retrievers'].items():
            if retriever_status['ready']:
                ready_retrievers.append(name.upper())
        
        logger.info(f"âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³: {', '.join(ready_retrievers)}")
        
        if watch_success:
            logger.info("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è‡ªå‹•ç›£è¦–: æœ‰åŠ¹")
        else:
            logger.warning("âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è‡ªå‹•ç›£è¦–: ç„¡åŠ¹ï¼ˆæ‰‹å‹•æ›´æ–°ã®ã¿ï¼‰")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False

# FastMCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½œæˆ
mcp = FastMCP("Hybrid Search Server")

@mcp.tool
def hybrid_search(
    query: str,
    mode: str = "hybrid", 
    max_results: int = 3,
    bm25_weight: float = 1.0,
    vector_weight: float = 1.0
) -> str:
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ ("hybrid", "bm25", "vector")
        max_results: æœ€å¤§çµæœæ•° (1-50)
        bm25_weight: BM25æ¤œç´¢ã®é‡ã¿ (0.1-2.0)
        vector_weight: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿ (0.1-2.0)
    
    Returns:
        æ¤œç´¢çµæœã®JSONæ–‡å­—åˆ—
    """
    if not search_engine:
        return "âŒ ã‚¨ãƒ©ãƒ¼: æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ” æ¤œç´¢å®Ÿè¡Œ: '{query}' (mode: {mode}, max_results: {max_results})")
        
        # å¼•æ•°æ¤œè¨¼
        if not query.strip():
            return "âŒ ã‚¨ãƒ©ãƒ¼: æ¤œç´¢ã‚¯ã‚¨ãƒªãŒç©ºã§ã™"
        
        if mode not in ["hybrid", "bm25", "vector"]:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: ç„¡åŠ¹ãªæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ '{mode}'. hybrid/bm25/vectorã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
        
        if not (1 <= max_results <= 50):
            return "âŒ ã‚¨ãƒ©ãƒ¼: max_resultsã¯1-50ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„"
        
        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        if mode == "hybrid":
            results = search_engine.search_hybrid(
                query=query,
                k=max_results,
                bm25_weight=bm25_weight,
                vector_weight=vector_weight
            )
        elif mode == "bm25":
            results = search_engine.search_bm25_only(query=query, k=max_results)
        elif mode == "vector":
            results = search_engine.search_vector_only(query=query, k=max_results)
        
        # å¿œç­”æ™‚é–“ã‚’è¨ˆç®—
        response_time = time.time() - start_time
        
        # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        results_data = {
            "success": True,
            "query": query,
            "mode": mode,
            "total_results": len(results),
            "response_time": round(response_time, 3),
            "auto_monitoring": index_watcher.running if index_watcher else False,
            "results": []
        }
        
        for i, result in enumerate(results, 1):
            result_dict = {
                "rank": i,
                "file_path": str(result.file_path),
                "file_name": result.file_path.name,
                "score": round(result.score, 4),
                "search_type": result.search_type,
                "text_preview": result.text[:200] + "..." if len(result.text) > 200 else result.text
            }
            results_data["results"].append(result_dict)
        
        logger.info(f"âœ… æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®çµæœ ({response_time:.3f}ç§’)")
        
        # JSONæ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
        return json.dumps(results_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        response_time = time.time() - start_time
        logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        error_result = {
            "success": False,
            "query": query,
            "mode": mode,
            "error": str(e),
            "response_time": round(response_time, 3)
        }
        
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool
def get_file_content(file_path: str) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨æ–‡ã‚’å–å¾—ã—ã¾ã™
    
    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹ï¼‰
    
    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®JSONæ–‡å­—åˆ—
    """
    if not search_engine:
        return "âŒ ã‚¨ãƒ©ãƒ¼: æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾—: '{file_path}'")
        
        # å¼•æ•°æ¤œè¨¼
        if not file_path.strip():
            return "âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒç©ºã§ã™"
        
        # ãƒ‘ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        target_path = Path(file_path)
        
        # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ã€ç›£è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã¨ã—ã¦è§£é‡ˆ
        if not target_path.is_absolute():
            # è¨­å®šã‹ã‚‰ç›£è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
            from core import hybrid_config as config
            target_path = config.WATCH_DIRECTORY / target_path
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        if not target_path.exists():
            return json.dumps({
                "success": False,
                "file_path": str(target_path),
                "error": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“",
                "response_time": round(time.time() - start_time, 3)
            }, ensure_ascii=False, indent=2)
        
        if not target_path.is_file():
            return json.dumps({
                "success": False,
                "file_path": str(target_path),
                "error": "æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“",
                "response_time": round(time.time() - start_time, 3)
            }, ensure_ascii=False, indent=2)
        
        # ã‚µãƒãƒ¼ãƒˆå½¢å¼ç¢ºèª
        from core import hybrid_config as config
        if target_path.suffix.lower() not in config.SUPPORTED_EXTENSIONS:
            return json.dumps({
                "success": False,
                "file_path": str(target_path),
                "error": f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚å¯¾å¿œå½¢å¼: {', '.join(config.SUPPORTED_EXTENSIONS)}",
                "response_time": round(time.time() - start_time, 3)
            }, ensure_ascii=False, indent=2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = target_path.stat().st_size
        if file_size > config.MAX_FILE_SIZE:
            return json.dumps({
                "success": False,
                "file_path": str(target_path),
                "error": f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™ï¼ˆ{file_size / (1024*1024):.1f}MB > {config.MAX_FILE_SIZE / (1024*1024):.1f}MBï¼‰",
                "response_time": round(time.time() - start_time, 3)
            }, ensure_ascii=False, indent=2)
        
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        base_system = search_engine.index_manager  # HybridBaseSystemã‚’ç¶™æ‰¿ã—ã¦ã„ã‚‹
        text_content = base_system.extract_text(target_path)
        
        if not text_content.strip():
            return json.dumps({
                "success": False,
                "file_path": str(target_path),
                "error": "ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
                "response_time": round(time.time() - start_time, 3)
            }, ensure_ascii=False, indent=2)
        
        # å¿œç­”æ™‚é–“ã‚’è¨ˆç®—
        response_time = time.time() - start_time
        
        # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        result_data = {
            "success": True,
            "file_path": str(target_path),
            "file_name": target_path.name,
            "file_size": file_size,
            "file_extension": target_path.suffix.lower(),
            "text_length": len(text_content),
            "content": text_content,
            "response_time": round(response_time, 3)
        }
        
        logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾—å®Œäº†: {target_path.name} ({len(text_content)}æ–‡å­—, {response_time:.3f}ç§’)")
        
        # JSONæ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
        return json.dumps(result_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        response_time = time.time() - start_time
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        error_result = {
            "success": False,
            "file_path": file_path,
            "error": str(e),
            "response_time": round(response_time, 3)
        }
        
        return json.dumps(error_result, ensure_ascii=False, indent=2)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="FastMCP Hybrid Search MCP Server"
    )
    
    parser.add_argument(
        "--host",
        default="127.0.0.1",
        help="MCPã‚µãƒ¼ãƒãƒ¼ã®ãƒ›ã‚¹ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 127.0.0.1)"
    )
    
    parser.add_argument(
        "--port", "-p",
        type=int,
        default=8000,
        help="MCPã‚µãƒ¼ãƒãƒ¼ã®ãƒãƒ¼ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8000)"
    )
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ Hybrid Search MCP Server (FastMCP) èµ·å‹•ä¸­...")
    logger.info(f"ğŸŒ ã‚µãƒ¼ãƒãƒ¼è¨­å®š: {args.host}:{args.port}")
    
    # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    if not initialize_search_engine():
        logger.error("âŒ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    logger.info("ğŸ“¡ MCP ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™")
    
    try:
        # FastMCPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹• (argparseã§æŒ‡å®šã•ã‚ŒãŸè¨­å®š)
        mcp.run(host=args.host, port=args.port, transport="http")
    
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ã‚µãƒ¼ãƒãƒ¼ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’åœæ­¢
        if index_watcher:
            index_watcher.stop_watching()
            
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’åœæ­¢
        if index_watcher:
            index_watcher.stop_watching()
            
        sys.exit(1)

if __name__ == "__main__":
    main() 